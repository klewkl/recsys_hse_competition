{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 18:10:55.937226: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 18:10:55.937338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "BbYjKp9sZJk9Dkb9MowPMb",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "events = pd.read_csv('events.csv')\n",
    "item_features = pd.read_csv('item_features.csv')\n",
    "user_features = pd.read_csv('user_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.sort_values(by = 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:13<00:00, 457.28it/s]\n"
     ]
    }
   ],
   "source": [
    "X_input, y_target = [], []\n",
    "for i in tqdm(events.user_id.unique()):\n",
    "    if len(events[events.user_id == i].iloc[-20:-10].item_id) == 10:\n",
    "        X_input.append(list(events[events.user_id == i].iloc[-20:-10].item_id.values))\n",
    "    else:\n",
    "        lst = list(events[events.user_id == i].iloc[-20:-10].item_id.values)\n",
    "        while len(lst) != 10:\n",
    "            lst.insert(0, 0)\n",
    "        X_input.append(lst)\n",
    "    y_target.append(list(events[events.user_id == i].iloc[-10:].item_id.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:08<00:00, 696.44it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for i in tqdm(events.user_id.unique()):\n",
    "    if len(events[events.user_id == i].iloc[-10:].item_id) == 10:\n",
    "        X_test.append(list(events[events.user_id == i].iloc[-10:].item_id.values))\n",
    "    else:\n",
    "        lst = list(events[events.user_id == i].iloc[-10:].item_id.values)\n",
    "        while len(lst) != 10:\n",
    "            lst.insert(0, 0)\n",
    "        X_test.append(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input, y_target, X_test = np.array(X_input), np.array(y_target), np.array(X_test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_input, y_target, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, Embedding, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class BERT4RTB(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(BERT4RTB, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"), \n",
    "            tf.keras.layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_bert4rec_model(num_features, embed_dim, num_heads, ff_dim, mask_rate=0.15):\n",
    "    inputs = Input(shape=(num_features,))  \n",
    "\n",
    "    embedding_layer = Dense(embed_dim, activation='relu')(inputs) \n",
    "    x = tf.expand_dims(embedding_layer, axis=1) \n",
    "\n",
    "    positional_embedding = tf.range(start=0, limit=num_features, delta=1)\n",
    "    pos_embedding_layer = Embedding(input_dim=num_features, output_dim=embed_dim)\n",
    "    pos_encoding = pos_embedding_layer(positional_embedding)\n",
    "    x += tf.expand_dims(pos_encoding, axis=0)  \n",
    "\n",
    "    mask = tf.random.uniform(shape=tf.shape(x), minval=0, maxval=1) < mask_rate\n",
    "    x = tf.where(mask, 0.0, x)  \n",
    "\n",
    "    transformer_block = BERT4RTB(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x) \n",
    "\n",
    "    x = tf.squeeze(x, axis=1)\n",
    "\n",
    "    x = Dense(9000, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "max_sequence_length = 20  \n",
    "n_preds = 10   \n",
    "num_users = 6040 \n",
    "num_films = events.item_id.max()+1  \n",
    "def create_film_recommendation_model(num_users, events.item_id.max()+1 , embed_dim, num_heads, ff_dim, sequence_length, n_preds):\n",
    "    user_input = Input(shape=(1,), name=\"user_input\")\n",
    "    film_sequence_input = Input(shape=(sequence_length - n_preds,), name=\"film_sequence_input\")\n",
    "\n",
    "    user_embedding = Embedding(input_dim=num_users, output_dim=embed_dim, name=\"user_embedding\")(user_input)\n",
    "    film_embedding = Embedding(input_dim=events.item_id.max()+1 , output_dim=embed_dim, name=\"film_embedding\")(film_sequence_input)\n",
    "    \n",
    "    positional_encoding = Embedding(input_dim=sequence_length, output_dim=embed_dim, name=\"positional_encoding\")\n",
    "    positions = tf.range(start=0, limit=(sequence_length - n_preds), delta=1)\n",
    "    pos_encoding = positional_encoding(positions)\n",
    "    film_embedding += pos_encoding  \n",
    "\n",
    "    transformer_block = BERT4RTBBERT4RecTransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(film_embedding)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(9000, activation='relu')(x)\n",
    "    outputs = Dense(events.item_id.max()+1  * n_preds, activation='softmax')(x)  \n",
    "    \n",
    "    outputs = tf.reshape(outputs, (-1, n_preds, events.item_id.max()+1 ))\n",
    "    \n",
    "    model = Model(inputs=[user_input, film_sequence_input], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "embed_dim = 8  \n",
    "num_heads = 4   \n",
    "ff_dim = 32     \n",
    "model = create_film_recommendation_model(num_users, events.item_id.max()+1 , embed_dim, num_heads, ff_dim, 20, n_preds)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  \n",
    "    metrics='accuracy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "180/180 [==============================] - 188s 1s/step - loss: 7.7896 - accuracy: 0.0038 - val_loss: 7.5661 - val_accuracy: 0.0043\n",
      "Epoch 2/5\n",
      "180/180 [==============================] - 185s 1s/step - loss: 7.0753 - accuracy: 0.0064 - val_loss: 7.6007 - val_accuracy: 0.0036\n",
      "Epoch 3/5\n",
      "180/180 [==============================] - 222s 1s/step - loss: 6.5033 - accuracy: 0.0094 - val_loss: 8.0007 - val_accuracy: 0.0066\n",
      "Epoch 4/5\n",
      "180/180 [==============================] - 198s 1s/step - loss: 5.8109 - accuracy: 0.0182 - val_loss: 8.5797 - val_accuracy: 0.0060\n",
      "Epoch 5/5\n",
      "180/180 [==============================] - 180s 1s/step - loss: 5.1454 - accuracy: 0.0398 - val_loss: 9.3117 - val_accuracy: 0.0043\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train[:, 0], X_train], y_train, validation_data=([X_val[:, 0], X_val], y_val), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 19s 102ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([X_test[:, 0], X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv('submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 1 2 3 4 5 6 7 8 9'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.item_id.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:00<00:00, 44534.83it/s]\n"
     ]
    }
   ],
   "source": [
    "subm = {'user_id':[], 'item_id':[]}\n",
    "count = 0\n",
    "for i in tqdm(events.user_id.unique()):\n",
    "    subm['user_id'].append(i)\n",
    "    res = ''\n",
    "    for j in preds[count]:\n",
    "        res+=str(j.argmax())\n",
    "        res+=' '\n",
    "    count += 1\n",
    "    subm['item_id'].append(res[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id 12080\n",
      "item_id 6040\n"
     ]
    }
   ],
   "source": [
    "for i in subm:\n",
    "    print(i, len(subm[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = pd.DataFrame(subm)\n",
    "subm = subm.sort_values(by = 'user_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv('the_greatest_submussion(no).csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
